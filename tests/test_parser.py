# -*- coding: UTF-8 -*-

import pytest
import os.path, sys
sys.path.append(os.path.join(os.path.dirname(os.path.realpath(__file__)), os.pardir))
from lara import nlp, parser

@pytest.mark.parametrize("intents,text,match", [
	(	
		[
			{
				"alma"			: [{"stem":"alma","wordclass":"noun"}],
				"szed"			: [{"stem":"szed","wordclass":"verb"}]
			},
			{
				"piros"			: [{"stem":"piros","wordclass":"adjective"}]
			},
			{
				"zold"			: [{"stem":"z√∂ld","wordclass":"adjective"}]
			}
		],
		[
			"M√°r a z√∂ld alm√°kat is szedjem le, vagy cask a pirosakat?",
			"M√°r a z√∂ld alm√°kat is szedjem le, vagy cask a pirosakat?",
			"M√°r a z√∂ld alm√°kat is szedjem le, vagy cask a pirosakat?",
			"M√°r a z√∂ld alm√°kat is szedjem le, vagy cask a pirosakat?"
		],
		[
			{'alma': 1, 'szed': 2},
			{'alma': 1, 'szed': 2, 'piros': 2},
			{'alma': 1, 'szed': 2, 'piros': 2, 'zold': 2},
			{'alma': 1, 'szed': 2, 'piros': 2, 'zold': 2}
		]
	),
])
def test_parser_intents_add(intents,text,match):
	test	= []
	test.append(parser.Intents(intents[0]))
	test.append(test[0]+parser.Intents(intents[1]))
	test.append(test[1]+intents[2])
	test.append(parser.Intents(str(test[2]),True))
	for i in range(len(text)):
		result	= test[i].match(text[i])
		assert match[i] == result
		
@pytest.mark.parametrize("intent,text,match", [
    (	
		{
			"alma"			: [{"stem":"alma","wordclass":"noun"}],
			"szed"			: [{"stem":"szed","wordclass":"verb"}],
			"piros"			: [{"stem":"piros","wordclass":"adjective"}]
		},
		[
			"Mikor szedj√ºk le a pirosabb alm√°kat?"
		],
		[
			{'alma': 1, 'szed': 2, 'piros': 2}
		]
	),
	(
		{
			"to_do"			: [{"stem":"csin√°l","wordclass":"verb"}],
		},
		[
			"≈ê mit csin√°l a szob√°ban?",
			"Mit fogok m√©g csin√°lni?",
			"Mikor csin√°ltad meg a szekr√©nyt?",
			"Megcsin√°ltatt√°k a berendez√©st.",
			"Teljesen kicsin√°lva √©rzem magamat ett≈ël a melegt≈ël.",
			"Csin√°lhatott volna m√°st is.",
			"Visszacsinalnad az ekezeteket a billentyuzetemen, kerlek?"
		],
		[
			{'to_do': 2},
			{'to_do': 2},
			{'to_do': 2},
			{'to_do': 2},
			{'to_do': 2},
			{'to_do': 2},
			{'to_do': 1}
		]
	),
	(
		{
			"palyaudvar"	: [{"stem":"p√°lyaudvar","wordclass":"noun","prefix":["busz"]}],
			"auto"			: [{"stem":"aut√≥","wordclass":"noun","affix":["busz"]}],
			"szinten_jo"	: [{"stem":"p√°lya","wordclass":"noun","prefix":["busz"],"affix":["udvar"]}]
		},
		[
			"Lassan be√©r√ºnk az aut√≥val a p√°lyaudvarra.",
			"Lassan be√©r√ºnk az aut√≥busszal a buszp√°lyaudvarra."
		],
		[
			{'palyaudvar': 2, 'auto': 2, 'szinten_jo': 2},
			{'palyaudvar': 2, 'auto': 1, 'szinten_jo': 2}
		]
	),
	(
		{
			"enni"		: [{"stem":"esz","wordclass":"verb","match_stem":False},{"stem":"en","wordclass":"verb","match_stem":False}]
		},
		[
			"T≈ëmorf√©m√°k: esz, en.",
			"Eszel valamit?",
			"Azt nem lehet megenni."
		],
		[
			{},
			{'enni': 2},
			{'enni': 2}
		]	
	),
	(
		{
			"jo_ido"	: [{"stem":"j√≥","wordclass":"adjective","with":[{"stem":"id≈ë","wordclass":"noun","affix":["j√°r√°s"]},{"stem":"meleg","wordclass":"adjective"}]}]
		},
		[
			"J√≥.",
			"Meleg van.",
			"Milyen az id≈ëj√°r√°s?",
			"J√≥ meleg van.",
			"J√≥ az id≈ë.",
			"J√≥ meleg az id≈ë.",
			"J√≥ meleg az id≈ëj√°r√°s."
		],
		[
			{},
			{},
			{},
			{'jo_ido': 2},
			{'jo_ido': 2},
			{'jo_ido': 4},
			{'jo_ido': 4}
		]
	),
	(
		{
			"jobb_ido"	: [{"stem":"j√≥","wordclass":"adjective",
							"with":[{"stem":"id≈ë","wordclass":"noun","affix":["j√°r√°s"]},{"stem":"meleg","wordclass":"adjective"}],
							"without":[{"stem":"este","wordclass":"noun"}]}]
		},
		[
			"J√≥.",
			"J√≥ meleg az id≈ëj√°r√°s.",
			"J√≥ est√©t!",
			"J√≥ meleg est√©nk van!"
		],
		[
			{},
			{'jobb_ido': 4},
			{},
			{}
		]
	),
	(
		{
			"megszerel"	: [{"stem":"szerel","wordclass":"verb"}],
			"hibasan"	: [{"stem":"alma","wordclass":"noun"}],
		},
		[
			"Gy√∂ny√∂r≈± dolog a szerelem",
			"Ezt is elfogadja tal√°latk√©nt: Almain√ºdb√∂z"
		],
		[
			{'megszerel': 2},
			{'hibasan': 2}
		]
	),
	(
		{
			"float"	: [{"stem":"a","score":.75},{"stem":"b","score":.6,"typo_score":1}],
		},
		[
			"a b c"
		],
		[
			{'float': 3.1},
		]
	),
])
def test_parser_intents_match(intent,text,match):
	test	= parser.Intents(intent)
	for i in range(len(text)):
		result	= test.match(text[i])
		assert match[i] == result

@pytest.mark.parametrize("intent,text,match", [
	(
		{
			"change"	: [{"stem":"sz√©p","wordclass":"adjective"}],
			"typo"		: [{"stem":"g√∂rbe","wordclass":"adjective"}],
			"fail"		: [{"stem":"k√©k","wordclass":"adjective"}]
		},
		[
			"Szebb s√°rga b√∂gre g√∂bre b√∂gre."
		],
		[
			['change','typo']
		]
	),
	(
		{
			"capital"	: [{"stem":"NAGY","wordclass":"adjective","ignorecase":False}],
			"lower"		: [{"stem":"kicsi","wordclass":"adjective","ignorecase":False}],
			"any"		: [{"stem":"V√°LtAkOz√ì","wordclass":"adjective","ignorecase":True}],
			"acr"		: [{"stem":"KFT","ignorecase":False}]
		},
		[
			"legesLEGNAGYobb kicsiNEK v√ÅlTaKoZ√≥ sz√∂veg kft"
		],
		[
			['capital','lower','any']
		]
	),
	(
		{
			"szoto"		: [{"stem":"t√∂v","wordclass":"noun","match_stem":False,"prefix":["sz√≥"]}],
			"ragoz"		: [{"stem":"ragozatlan","wordclass":"adjective","match_stem":False}],
			"talal"		: [{"stem":"tal√°l","wordclass":"verb","match_stem":False}],
			"talan"		: [{"stem":"TAL√ÅN","wordclass":"verb","match_stem":False,"ignorecase":False}]
		},
		[
			"SZ√ìT√ñVEK SZ√ìT√ñVEK SZ√ìT√ñVEK",
			"Sz√≥t√∂vet RAGOZATLANUL nem tal√°l meg. TAL√ÅN √≠gy?",
			"Ebben semmi sincs"
		],
		[
			['szoto'],
			['szoto','ragoz'],
			[]
		]
	)
])
def test_parser_intents_match_set(intent,text,match):
	test	= parser.Intents(intent)
	for i in range(len(text)):
		result	= test.match_set(text[i])
		assert set(match[i]) == result

@pytest.mark.parametrize("intent,text,best", [
	(	
		{
			"kave"			: [{"stem":"k√°v√©","wordclass":"noun","affix":["g√©p"]}],
			"takarit"		: [{"stem":"takar√≠t","wordclass":"verb"}],
			"sehol"			: [{"stem":"sehol"}]
		},
		[
			"Valakinek ki kellene takar√≠tani a k√°v√©g√©pet. Tegnap is √©n takar√≠tottam ki.",
			"K√°v√© k√°v√©t k√°v√©nk k√°v√©m. Takar√≠t.",
			"K√°v√© k√°v√©t k√°v√©nk k√°v√©m. Takar√≠t."
		],
		[
			{'takarit': 4},
			{'kave': 8, 'takarit': 2},
			{'kave': 8, 'takarit': 2}
		]
	),
])
def test_parser_intents_match_best(intent,text,best):
	test	= parser.Intents(intent)
	for i in range(len(text)):
		result	= test.match_best(text[i],i+1)
		assert best[i] == result
		
@pytest.mark.parametrize("intents,text,cleaned", [
	(
		[
			{
				"thanks"	: [{"stem":"k√∂sz√∂n","wordclass":"verb"}]
			},
			{
				"thanks"	: [{"stem":"k√∂sz√∂n","wordclass":"verb","without":[{"stem":"sz√©p","wordclass":"adjective"}]}]
			},
			{
				"thanks"	: [{"stem":"k√∂sz√∂n","wordclass":"verb","with":[{"stem":"nagy","wordclass":"adjective"}]}]
			},
			{
				"thanks"	: [{"stem":"k√∂sz√∂n","wordclass":"verb","with":[{"stem":"kicsi","wordclass":"adjective"}]}]
			},
			{
				"thanks"	: [{"stem":"k√∂sz√∂n","wordclass":"verb","with":[{"stem":"nagy","wordclass":"adjective"}],"without":[{"stem":"sz√©p","wordclass":"adjective"}]}]
			}
		],
		[
			"Nagyon sz√©pen k√∂sz√∂n√∂m a teszteket!",
			"Nagyon sz√©pen k√∂sz√∂n√∂m a teszteket!",
			"Nagyon sz√©pen k√∂sz√∂n√∂m a teszteket!",
			"Nagyon sz√©pen k√∂sz√∂n√∂m a teszteket!",
			"Nagyon sz√©pen k√∂sz√∂n√∂m a teszteket!"
		],
		[
			"Nagyon sz√©pen a teszteket!",
			"Nagyon sz√©pen k√∂sz√∂n√∂m a teszteket!",
			"Nagyon sz√©pen a teszteket!",
			"Nagyon sz√©pen k√∂sz√∂n√∂m a teszteket!",
			"Nagyon sz√©pen k√∂sz√∂n√∂m a teszteket!"
		]
	)
])
def test_parser_intents_clean(intents,text,cleaned):
	for i in range(len(intents)):
		test	= parser.Intents(intents[i])
		result	= nlp.trim(test.clean(text[i]))
		assert cleaned[i] == result
		
@pytest.mark.parametrize("info", [
	(
		{
			"text"		: "teszt sz√∂veg"			
		}
	),
	(
		{
			"text"		: "teszt sz√∂veg https://www.youtube.com/watch?v=dQw4w9WgXcQ",
			"urls"		: ["https://www.youtube.com/watch?v=dQw4w9WgXcQ"],
			"smileys"	: ["=d"]
		}
	),
	(
		{
			"text"		: "@mention √©s #hashtag",
			"mentions"	: ["@mention"],
			"hashtags"	: ["#hashtag"],
		}
	),
	(
		{
			"text"		: "@mention √©s #hashtag D:",
			"mentions"	: ["@mention"],
			"hashtags"	: ["#hashtag"],
			"smileys"	: ["D:"],
		}
	),
	(
		{
			"text"		: ":DDDDdddd :(((8888 :3 http://",
			"smileys"	: [":DDDDdddd",":(((8888",":3"]
		}
	),
	(
		{
			"text"		: "18/01/09 vagy 18-01-09 vagy 2018. 01. 09. vagy 2018. 01. 09-√©n vagy 2018 janu√°r 19-√©n",
			"dates"		: ["18/01/09","18-01-09","2018. 01. 09","2018. 01. 09","2018 janu√°r 19"],
		}
	),
	(
		{
			"text"		: "$5 000 vagy 5 000$ vagy 5000 doll√°r 5000.-",
			"currencies": ["$5 000","5 000$","5000 doll√°r","5000.-"],
		}
	),
	(
		{
			"text"		: "$√ü≈Å≈Ç üçπ-üòÉüçî :) √ü¬§√©$√ó asddasd",
			"emojis"	: ["üçπ","üòÉ","üçî"],
			"smileys"	: [":)"]
		}
	)
])
def test_parser_extract(info):
	test	= parser.Extract(info['text'])
	check	= ['hashtags','mentions','urls','smileys','dates','currencies','emojis']
	for item in info:
		if item!='text' and item not in check:
			raise ValueError('Possible typo in test case:',item)
	for item in check:
		result	= eval('test.'+item+'()')
		if item in info:
			assert set(info[item]) == set(result)
		else:
			assert not result